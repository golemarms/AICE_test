{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlp\n",
    "import pandas\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## Settings from config.yaml\n",
    "\n",
    "ml_config_dict = mlp.setup.load_config()['ml_config']\n",
    "verbosity = mlp.setup.load_config()['verbosity']\n",
    "\n",
    "def make_XY(df):\n",
    "    X = df.drop(columns=['total_scooter'])\n",
    "    Y = df['total_scooter']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def sklearn_create(input_string, kwarg_dict):\n",
    "    \"\"\"\n",
    "    Given an input string, instantiates object from sklearn api library\n",
    "    \"\"\"\n",
    "    if kwarg_dict:\n",
    "        return eval(f\"{input_string}\")(**kwarg_dict)\n",
    "    else:\n",
    "        return eval(f\"{input_string}\")()\n",
    "\n",
    "class Pipeline(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initializing ML pipeline...\")\n",
    "        \n",
    "        ## Initialize models\n",
    "        self.reg = sklearn_create(ml_config_dict['chosen_regressor'], ml_config_dict['regressor_params'])\n",
    "        self.scaler = sklearn_create(ml_config_dict['chosen_scaler'], ml_config_dict['scaler_params'])\n",
    "        \n",
    "        ## Extract, preprocess and split data\n",
    "        self.train_df, self.test_df = mlp.preprocessing.extract_preprocess_split()\n",
    "        self.X_train, self.Y_train = make_XY(self.train_df)\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        \n",
    "        ## Add params to init_dict dictionary for logging purposes\n",
    "        self.init_dict = {}\n",
    "        self.init_dict['datetime'] = str(datetime.datetime.now())\n",
    "        self.init_dict['model'] = ml_config_dict['chosen_regressor']\n",
    "        self.init_dict['model_params_user'] = ml_config_dict['regressor_params']\n",
    "        self.init_dict['scaler'] = ml_config_dict['chosen_scaler']\n",
    "        self.init_dict['scaler_params_user'] = ml_config_dict['scaler_params']\n",
    "        \n",
    "        ## Initialize test_dict logging purposes\n",
    "        self.test_dict = {}\n",
    "        \n",
    "        return\n",
    "\n",
    " \n",
    "    def grid_search(self):\n",
    "        self.grid_search = GridSearchCV(self.reg, ml_config_dict['param_grid'],\n",
    "                                       cv=ml_config_dict['cv_folds'],\n",
    "                                       scoring='neg_mean_squared_error',\n",
    "                                       return_train_score=True,\n",
    "                                       verbose=verbosity,\n",
    "                                       refit=True)\n",
    "\n",
    "        print(\"Starting grid search...\")\n",
    "        self.grid_search.fit(self.X_train_scaled, self.Y_train)\n",
    "        print(\"Grid search complete\")\n",
    "        self.cv_results = self.grid_search.cv_results_\n",
    "        self.final_model = self.grid_search.best_estimator_\n",
    "        \n",
    "        best_index = self.grid_search.best_index_\n",
    "        \n",
    "        # Metrics to extract from cv_results\n",
    "        eval_metrics = ['mean_train_score', 'mean_test_score' , 'params']\n",
    "\n",
    "        self.best_metrics = {metric:score[best_index] for metric,score in self.cv_results.items() if metric in eval_metrics}\n",
    "        self.best_metrics[\"rmse_train\"] = np.sqrt(-self.best_metrics[\"mean_train_score\"])\n",
    "        self.best_metrics[\"rmse_cv\"] = np.sqrt(-self.best_metrics[\"mean_test_score\"])\n",
    "\n",
    "        del self.best_metrics['mean_train_score']\n",
    "        del self.best_metrics['mean_test_score']\n",
    "        self.best_metrics[\"model_params_gridsearch\"] = self.best_metrics.pop(\"params\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def predict_test(self):\n",
    "        print(\"Evaluating on test set...\")\n",
    "        X_test, Y_test = make_XY(self.test_df)\n",
    "        X_test_scaled=self.scaler.transform(X_test)\n",
    "        \n",
    "        Y_pred_test = self.final_model.predict(X_test_scaled)\n",
    "        self.test_dict[\"rmse_test\"] = np.sqrt(mean_squared_error(Y_test, Y_pred_test))\n",
    "    \n",
    "    def get_combined_dict(self):\n",
    "        return {**self.init_dict, **self.best_metrics, **self.test_dict}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
